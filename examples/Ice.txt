	Ice: non-parametric smoothing in
			an age-cohort model

Breslow and Clayton (1993) analyse  breast cancer rates in Iceland by year of birth (K = 11 cohorts from 1840-1849 to 1940-1949) and by age (J =13 groups from 20-24 to 80-84 years).  Due to the number of empty cells we consider a single indexing over I = 77 observed number of cases, giving data of the following form.



	i	agei	yeari	casesi	person-yearsi
	_____	________________________________________
	1	1	6	2	41380
	2	1	7	0	43650
	...	...
	77	13	5	31	13600

In order to pull in the extreme risks associated with small birth cohorts, Breslow and
Clayton first consider the exchangeable model


	casesi	~	Poisson(mi)
	log mi	=	log  person-yearsi + aagei + byeari
	bk	~	Normal( 0, t )
 
Autoregressive smoothing of relative risks
 
They then consider the alternative approach of smoothing the rates for the cohorts by assuming an auto-regressive model on the b's, assuming the second differences are independent normal variates.  This is equivalent to a model and prior distribution 
	casesi	~	Poisson(mi)
	log mi	=	log  person-yearsi + aagei + byeari
	b1	~	Normal( 0, 0.000001t )
	b2 | b1	~	Normal( 0, 0.000001t )
	bk | b1,...,k-1	~	Normal( 2 bk-1- bk-2, t )      k > 2
		
We note that b1 and b2 are given "non-informative" priors, but retain a t term in order to provide the appropriate likelihood for t.

For computational reasons Breslow and Clayton impose constraints on their random effects bk in order that their mean and linear trend are zero, and counter these constraints by introducing a linear term b x yeari and allowing unrestrained estimation of aj.  Since we allow free movement of the b's we dispense with the linear term, and impose a "corner" constraint a1 =0 .  


	model 
	{
		for (i in 1:I)  {
			cases[i]        ~ dpois(mu[i])
			log(mu[i])     <- log(pyr[i]) + alpha[age[i]] + beta[year[i]]
			#cumulative.cases[i] <- cumulative(cases[i], cases[i])
		}
		betamean[1]    <- 2 * beta[2] - beta[3]
		Nneighs[1]     <- 1
		betamean[2]    <- (2 * beta[1] + 4 * beta[3] - beta[4]) / 5
		Nneighs[2]     <- 5
		for (k in 3 : K - 2)  {
			betamean[k]    <- (4 * beta[k - 1] + 4 * beta[k + 1]- beta[k - 2] - beta[k + 2]) / 6
			Nneighs[k]     <- 6
		}
		betamean[K - 1]  <- (2 * beta[K] + 4 * beta[K - 2] - beta[K - 3]) / 5
		Nneighs[K - 1]   <- 5
		betamean[K]    <- 2 * beta[K - 1] - beta[K - 2]  
		Nneighs[K]     <- 1
		for (k in 1 : K)  {
			betaprec[k]    <- Nneighs[k] * tau
		}
		for (k in 1 : K)  {
			beta[k]        ~ dnorm(betamean[k], betaprec[k])
			logRR[k]      <- beta[k] - beta[5]
			tau.like[k]   <- Nneighs[k] * beta[k] * (beta[k] - betamean[k])
		}
		alpha[1]      <- 0.0
		for (j in 2 : Nage)  {
			alpha[j]       ~ dnorm(0, 1.0E-6)
		}
		d <- 0.0001 + sum(tau.like[]) / 2
		r <- 0.0001 + K / 2
		tau  ~ dgamma(r, d)
		sigma <- 1 / sqrt(tau)
	}

Data ( click to open )
     
Inits for chain 1		Inits for chain 2 ( click to open )

Results 

A 1000 update burn in followed by a further 100000 updates gave the parameter estimates

		mean	sd	MC_error	val2.5pc	median	val97.5pc	start	sample
	logRR[1]	-1.038	0.252	0.0143	-1.577	-1.004	-0.6829	1001	20000
	logRR[2]	-0.748	0.1608	0.009239	-1.082	-0.7317	-0.5123	1001	20000
	logRR[3]	-0.4615	0.08272	0.004348	-0.6405	-0.452	-0.3355	1001	20000
	logRR[4]	-0.2006	0.03653	0.001021	-0.2758	-0.1982	-0.1241	1001	20000
	logRR[6]	0.1616	0.04168	0.001625	0.0552	0.1719	0.2206	1001	20000
	logRR[7]	0.3217	0.06385	0.003012	0.1724	0.3354	0.4188	1001	20000
	logRR[8]	0.4837	0.0803	0.004211	0.3024	0.4964	0.6134	1001	20000
	logRR[9]	0.6428	0.1036	0.005897	0.4178	0.6574	0.8112	1001	20000
	logRR[10]	0.819	0.1293	0.007686	0.5361	0.839	1.035	1001	20000
	logRR[11]	1.004	0.1757	0.009986	0.6182	1.026	1.302	1001	20000
	sigma	0.04657	0.03926	0.002001	0.006204	0.03607	0.1476	1001	20000

